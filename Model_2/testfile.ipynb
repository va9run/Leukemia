{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import *\n",
    "from PIL import Image\n",
    "import os\n",
    "import cv2\n",
    "from keras import preprocessing\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from skimage import exposure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingDataHEM = 'C:/Users/v4run/OneDrive/Desktop/Datasets/C-NMC_Leukemia/training_data/fold_1/hem'\n",
    "trainingDataALL = 'C:/Users/v4run/OneDrive/Desktop/Datasets/C-NMC_Leukemia/training_data/fold_1/all'\n",
    "\n",
    "\n",
    "validationDataHEM = 'C:/Users/v4run/OneDrive/Desktop/Datasets/C-NMC_Leukemia/training_data/fold_0/hem'\n",
    "validationDataALL = 'C:/Users/v4run/OneDrive/Desktop/Datasets/C-NMC_Leukemia/training_data/fold_0/all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_imlist(path):\n",
    "    return [os.path.join(path,f) for f in os.listdir(path) if f.endswith('.bmp')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataImport(path,pathType):\n",
    "    imageList = []\n",
    "    for i,j in zip(pathType,path):\n",
    "        if i not in ['ALL','HEM']:\n",
    "            raise ValueError(\"Entry must be either 'ALL' or 'HEM'\")\n",
    "        else:\n",
    "            if i == 'HEM':\n",
    "                loadPath = get_imlist(j) \n",
    "                for paths in loadPath:\n",
    "                    imageList.append({'Images':Image.open(paths),'Labels':0})\n",
    "            else:\n",
    "                loadPath = get_imlist(j) \n",
    "                for paths in loadPath:\n",
    "                    imageList.append({'Images':Image.open(paths),'Labels':1})\n",
    "    imageData = pd.DataFrame(imageList)\n",
    "    return imageData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_labels_from_df(path, pathType, img_size):\n",
    "\n",
    "    df = dataImport(path,pathType)\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        label = row['Labels']\n",
    "        img_data = row['Images']\n",
    "        img_resized = img_data.resize((img_size, img_size)) #resize images (128*128, 256*256 etc)\n",
    "        img_array = exposure.equalize_adapthist(np.array(img_resized),clip_limit=0.03,nbins=256)  # Normalize pixel values\n",
    "        img_array = (img_array*255).astype(np.uint8)\n",
    "        images.append(img_array)\n",
    "        labels.append(label)\n",
    "    \n",
    "    images = np.array(images)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # crop images\n",
    "    cropped_images = []\n",
    "\n",
    "    for i in images:\n",
    "        cropped_image = i[10:-10, 10:-10, :] \n",
    "        cropped_images.append(cropped_image)\n",
    "\n",
    "    cropped_images = np.array(cropped_images)\n",
    "    cropped_images = cropped_images.astype('uint8')\n",
    "    \n",
    "    return cropped_images, labels, len(cropped_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augmentation(path, pathType, img_size):\n",
    "\n",
    "    cropped_images, labels, total_images = load_images_labels_from_df(path,pathType,img_size)\n",
    "    augmentation = preprocessing.image.ImageDataGenerator(\n",
    "                                                        rotation_range=50, # image rotation range\n",
    "                                                        width_shift_range=0.2, # horizontal shift during augmentation\n",
    "                                                        height_shift_range=0.2, # vertical shift during augmentation\n",
    "                                                        shear_range=0, # shear angle shift during augmentation\n",
    "                                                        zoom_range=0.2, # image zoom-in or out during augmentation\n",
    "                                                        horizontal_flip=True, \n",
    "                                                        fill_mode='nearest' # how to fill empty spaces created during augmentation*\n",
    "                                                    )\n",
    "    \n",
    "    augmentation.fit(cropped_images)\n",
    "    augmentedImage_generator = augmentation.flow(cropped_images, labels, batch_size=32)\n",
    "    return augmentedImage_generator,total_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData_generator, totalTrainImages = data_augmentation([trainingDataALL,trainingDataHEM],['ALL','HEM'],256)\n",
    "validationData_generator, labels, totalValidationImages = load_images_labels_from_df([validationDataALL,validationDataHEM],['ALL','HEM'],256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "c,d = next(trainingData_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x27fb4336760>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "111/111 [==============================] - ETA: 0s - loss: 2.6580 - accuracy: 0.6979WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 110 batches). You may need to use the repeat() function when building your dataset.\n",
      "111/111 [==============================] - 175s 2s/step - loss: 2.6580 - accuracy: 0.6979 - val_loss: 0.5911 - val_accuracy: 0.7284\n",
      "Epoch 2/50\n",
      "111/111 [==============================] - 141s 1s/step - loss: 0.5407 - accuracy: 0.7413\n",
      "Epoch 3/50\n",
      "111/111 [==============================] - 148s 1s/step - loss: 0.5225 - accuracy: 0.7650\n",
      "Epoch 4/50\n",
      "111/111 [==============================] - 161s 1s/step - loss: 0.4941 - accuracy: 0.7867\n",
      "Epoch 5/50\n",
      "111/111 [==============================] - 161s 1s/step - loss: 0.4829 - accuracy: 0.7915\n",
      "Epoch 6/50\n",
      "111/111 [==============================] - 159s 1s/step - loss: 0.4706 - accuracy: 0.7988\n",
      "Epoch 7/50\n",
      "111/111 [==============================] - 156s 1s/step - loss: 0.4829 - accuracy: 0.8042\n",
      "Epoch 8/50\n",
      "111/111 [==============================] - 155s 1s/step - loss: 0.4556 - accuracy: 0.8090\n",
      "Epoch 9/50\n",
      "111/111 [==============================] - 155s 1s/step - loss: 0.4433 - accuracy: 0.8076\n",
      "Epoch 10/50\n",
      "111/111 [==============================] - 155s 1s/step - loss: 0.4471 - accuracy: 0.8101\n",
      "Epoch 11/50\n",
      "111/111 [==============================] - 157s 1s/step - loss: 0.4470 - accuracy: 0.8118\n",
      "Epoch 12/50\n",
      "111/111 [==============================] - 159s 1s/step - loss: 0.4289 - accuracy: 0.8177\n",
      "Epoch 13/50\n",
      "111/111 [==============================] - 159s 1s/step - loss: 0.4263 - accuracy: 0.8146\n",
      "Epoch 14/50\n",
      "111/111 [==============================] - 158s 1s/step - loss: 0.4176 - accuracy: 0.8225\n",
      "Epoch 15/50\n",
      "111/111 [==============================] - 157s 1s/step - loss: 0.3994 - accuracy: 0.8352\n",
      "Epoch 16/50\n",
      "111/111 [==============================] - 155s 1s/step - loss: 0.4340 - accuracy: 0.8242\n",
      "Epoch 17/50\n",
      "111/111 [==============================] - 160s 1s/step - loss: 0.4154 - accuracy: 0.8295\n",
      "Epoch 18/50\n",
      "111/111 [==============================] - 160s 1s/step - loss: 0.4076 - accuracy: 0.8338\n",
      "Epoch 19/50\n",
      "111/111 [==============================] - 152s 1s/step - loss: 0.3966 - accuracy: 0.8453\n",
      "Epoch 20/50\n",
      "111/111 [==============================] - 153s 1s/step - loss: 0.3918 - accuracy: 0.8371\n",
      "Epoch 21/50\n",
      "111/111 [==============================] - 155s 1s/step - loss: 0.3857 - accuracy: 0.8405\n",
      "Epoch 22/50\n",
      "111/111 [==============================] - 155s 1s/step - loss: 0.3883 - accuracy: 0.8408\n",
      "Epoch 23/50\n",
      "111/111 [==============================] - 155s 1s/step - loss: 0.3936 - accuracy: 0.8385\n",
      "Epoch 24/50\n",
      "111/111 [==============================] - 155s 1s/step - loss: 0.4013 - accuracy: 0.8301\n",
      "Epoch 25/50\n",
      "111/111 [==============================] - 155s 1s/step - loss: 0.3947 - accuracy: 0.8343\n",
      "Epoch 26/50\n",
      "111/111 [==============================] - 155s 1s/step - loss: 0.3863 - accuracy: 0.8416\n",
      "Epoch 27/50\n",
      "111/111 [==============================] - 157s 1s/step - loss: 0.3742 - accuracy: 0.8447\n",
      "Epoch 28/50\n",
      "111/111 [==============================] - 156s 1s/step - loss: 0.3850 - accuracy: 0.8450\n",
      "Epoch 29/50\n",
      "111/111 [==============================] - 155s 1s/step - loss: 0.3838 - accuracy: 0.8470\n",
      "Epoch 30/50\n",
      "111/111 [==============================] - 152s 1s/step - loss: 0.3872 - accuracy: 0.8416\n",
      "Epoch 31/50\n",
      "111/111 [==============================] - 152s 1s/step - loss: 0.3851 - accuracy: 0.8442\n",
      "Epoch 32/50\n",
      "111/111 [==============================] - 152s 1s/step - loss: 0.3770 - accuracy: 0.8459\n",
      "Epoch 33/50\n",
      "111/111 [==============================] - 153s 1s/step - loss: 0.3648 - accuracy: 0.8518\n",
      "Epoch 34/50\n",
      "111/111 [==============================] - 156s 1s/step - loss: 0.3764 - accuracy: 0.8473\n",
      "Epoch 35/50\n",
      "111/111 [==============================] - 156s 1s/step - loss: 0.3693 - accuracy: 0.8431\n",
      "Epoch 36/50\n",
      "111/111 [==============================] - 155s 1s/step - loss: 0.3728 - accuracy: 0.8450\n",
      "Epoch 37/50\n",
      "111/111 [==============================] - 155s 1s/step - loss: 0.3676 - accuracy: 0.8493\n",
      "Epoch 38/50\n",
      "111/111 [==============================] - 154s 1s/step - loss: 0.3686 - accuracy: 0.8490\n",
      "Epoch 39/50\n",
      "111/111 [==============================] - 151s 1s/step - loss: 0.3640 - accuracy: 0.8478\n",
      "Epoch 40/50\n",
      "111/111 [==============================] - 153s 1s/step - loss: 0.3820 - accuracy: 0.8431\n",
      "Epoch 41/50\n",
      "111/111 [==============================] - 153s 1s/step - loss: 0.3666 - accuracy: 0.8540\n",
      "Epoch 42/50\n",
      "111/111 [==============================] - 153s 1s/step - loss: 0.3654 - accuracy: 0.8540\n",
      "Epoch 43/50\n",
      "111/111 [==============================] - 153s 1s/step - loss: 0.3690 - accuracy: 0.8473\n",
      "Epoch 44/50\n",
      "111/111 [==============================] - 153s 1s/step - loss: 0.3742 - accuracy: 0.8504\n",
      "Epoch 45/50\n",
      "111/111 [==============================] - 156s 1s/step - loss: 0.3656 - accuracy: 0.8549\n",
      "Epoch 46/50\n",
      "111/111 [==============================] - 157s 1s/step - loss: 0.3682 - accuracy: 0.8490\n",
      "Epoch 47/50\n",
      "111/111 [==============================] - 156s 1s/step - loss: 0.3616 - accuracy: 0.8555\n",
      "Epoch 48/50\n",
      "111/111 [==============================] - 156s 1s/step - loss: 0.3551 - accuracy: 0.8577\n",
      "Epoch 49/50\n",
      "111/111 [==============================] - 157s 1s/step - loss: 0.3653 - accuracy: 0.8490\n",
      "Epoch 50/50\n",
      "111/111 [==============================] - 157s 1s/step - loss: 0.3596 - accuracy: 0.8524\n",
      "107/110 [============================>.] - ETA: 0s - loss: 0.4848 - accuracy: 0.8305WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 110 batches). You may need to use the repeat() function when building your dataset.\n",
      "110/110 [==============================] - 33s 295ms/step - loss: 0.4848 - accuracy: 0.8305\n",
      "Validation loss:  0.4848 Validation accuracy:  0.8305\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32,(3,3), activation='relu',input_shape=(236, 236,3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64,(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(128,(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(\n",
    "    trainingData_generator,\n",
    "    steps_per_epoch=totalTrainImages//32,\n",
    "    epochs=50,\n",
    "    validation_data=(validationData_generator,labels),\n",
    "    validation_steps=totalValidationImages//32\n",
    ")\n",
    "\n",
    "loss, accuracy = model.evaluate(validationData_generator,labels,steps=totalValidationImages//32)\n",
    "print(f\"Validation loss: {loss: .4f}\", f\"Validation accuracy: {accuracy: .4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
